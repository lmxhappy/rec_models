{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine Deep Dive\n",
    "\n",
    "Factorization machine (FM) is one of the representative algorithms that are used for building hybrid recommenders model. The algorithm is powerful in terms of capturing the effects of not just the input features but also their interactions. The algorithm provides better generalization capability and expressiveness compared to other classic algorithms such as SVMs. The most recent research extends the basic FM algorithms by using deep learning techniques, which achieve remarkable improvement in a few practical use cases.\n",
    "\n",
    "This notebook presents a deep dive into the Factorization Machine algorithm, and demonstrates some best practices of using the contemporary FM implementations like [`xlearn`](https://github.com/aksnzhy/xlearn) for dealing with tasks like click-through rate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FM is an algorithm that uses factorization in prediction tasks with data set of high sparsity. The algorithm was original proposed in [\\[1\\]](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf). Traditionally, the algorithms such as SVM do not perform well in dealing with highly sparse data that is usually seen in many contemporary problems, e.g., click-through rate prediction, recommendation, etc. FM handles the problem by modeling not just first-order linear components for predicting the label, but also the cross-product of the feature variables in order to capture more generalized correlation between variables and label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain occasions, the data that appears in recommendation problems, such as user, item, and feature vectors, can be encoded into a one-hot representation. Under this arrangement, classical algorithms like linear regression and SVM may suffer from the following problems:\n",
    "1. The feature vectors are highly sparse, and thus it makes it hard to optimize the parameters to fit the model efficienly\n",
    "2. Cross-product of features will be sparse as well, and this in turn, reduces the expressiveness of a model if it is designed to capture the high-order interactions between features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://recodatasets.z20.web.core.windows.net/images/fm_data.png?sanitize=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FM algorithm is designed to tackle the above two problems by factorizing latent vectors that model the low- and high-order components. The general idea of a FM model is expressed in the following equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}(\\textbf{x})=w_{0}+\\sum^{n}_{i=1}w_{i}x_{i}+\\sum^{n}_{i=1}\\sum^{n}_{j=i+1}<\\textbf{v}_{i}, \\textbf{v}_{j}>x_{i}x_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{y}$ and $\\textbf{x}$ are the target to predict and input feature vectors, respectively. $w_{i}$ is the model parameters for the first-order component. $<\\textbf{v}_{i}, \\textbf{v}_{j}>$ is the dot product of two latent factors for the second-order interaction of feature variables, and it is defined as  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$<\\textbf{v}_{i}, \\textbf{v}_{j}>=\\sum^{k}_{f=1}v_{i,f}\\cdot v_{j,f}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to using fixed parameter for the high-order interaction components, using the factorized vectors increase generalization as well as expressiveness of the model. In addition to this, the computation complexity of the equation (above) is $O(kn)$ where $k$ and $n$ are the dimensionalities of the factorization vector and input feature vector, respectively (see [the paper](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) for detailed discussion). In practice, usually a two-way FM model is used, i.e., only the second-order feature interactions are considered to favor computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Field-Aware Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field-aware factorization machine (FFM) is an extension to FM. It was originally introduced in [\\[2\\]](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf). The advantage of FFM over FM is that, it uses different factorized latent factors for different groups of features. The \"group\" is called \"field\" in the context of FFM. Putting features into fields resolves the issue that the latent factors shared by features that intuitively represent different categories of information may not well generalize the correlation. \n",
    "\n",
    "Different from the formula for the 2-order cross product as can be seen above in the FM equation, in the FFM settings, the equation changes to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_{\\text{FFM}}(\\textbf{w}\\textbf{x})=\\sum^{n}_{j1=1}\\sum^{n}_{j2=j1+1}<\\textbf{v}_{j1,f2}, \\textbf{v}_{j2,f1}>x_{j1}x_{j2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $f_1$ and $f_2$ are the fields of $j_1$ and $j_2$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to FM, the computational complexity increases to $O(n^2k)$. However, since the latent factors in FFM only need to learn the effect within the field, so the $k$ values in FFM is usually much smaller than that in FM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 FM/FFM extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the recent years, FM/FFM extensions were proposed to enhance the model performance further. The new algorithms leverage the powerful deep learning neural network to improve the generalization capability of the original FM/FFM algorithms. Representatives of the such algorithms are summarized as below. Some of them are implemented and demonstrated in the microsoft/recommenders repository. \n",
    "\n",
    "|Algorithm|Notes|References|Example in Microsoft/Recommenders|\n",
    "|---------|-----|----------|---------------------------------|\n",
    "|DeepFM|Combination of FM and DNN where DNN handles high-order interactions|[\\[3\\]](https://arxiv.org/abs/1703.04247)|-|\n",
    "|xDeepFM|Combination of FM, DNN, and Compressed Interaction Network, for vectorized feature interactions|[\\[4\\]](https://dl.acm.org/citation.cfm?id=3220023)|[notebook](../00_quick_start/xdeepfm_criteo.ipynb) / [utilities](../../recommenders/models/deeprec/models/xDeepFM.py)|\n",
    "|Factorization Machine Supported Neural Network|Use FM user/item weight vectors as input layers for DNN model|[\\[5\\]](https://link.springer.com/chapter/10.1007/978-3-319-30671-1_4)|-|\n",
    "|Product-based Neural Network|An additional product-wise layer between embedding layer and fully connected layer to improve expressiveness of interactions of features across fields|[\\[6\\]](https://ieeexplore.ieee.org/abstract/document/7837964)|-|\n",
    "|Neural Factorization Machines|Improve the factorization part of FM by using stacks of NN layers to improve non-linear expressiveness|[\\[7\\]](https://dl.acm.org/citation.cfm?id=3080777)|-|\n",
    "|Wide and deep|Combination of linear model (wide part) and deep neural network model (deep part) for memorisation and generalization|[\\[8\\]](https://dl.acm.org/citation.cfm?id=2988454)|[notebook](../00_quick_start/wide_deep_movielens.ipynb) / [utilities](../../recommenders/models/wide_deep)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Factorization Machine Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table summarizes the implementations of FM/FFM. Some of them (e.g., xDeepFM and VW) are implemented and/or demonstrated in the microsoft/recommenders repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Implementation|Language|Notes|Examples in Microsoft/Recommenders|\n",
    "|-----------------|------------------|------------------|---------------------|\n",
    "|[libfm](https://github.com/srendle/libfm)|C++|Implementation of FM algorithm|-|\n",
    "|[libffm](https://github.com/ycjuan/libffm)|C++|Original implemenation of FFM algorithm. It is handy in model building, but does not support Python interface|-|\n",
    "|[xlearn](https://github.com/aksnzhy/xlearn)|C++ with Python interface|More computationally efficient compared to libffm without loss of modeling effectiveness|[notebook](fm_deep_dive.ipynb)|\n",
    "|[Vowpal Wabbit FM](https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Matrix-factorization-example)|Online library with estimator API|Easy to use by calling API|[notebook](../02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb) / [utilities](../../recommenders/models/vowpal_wabbit)\n",
    "|[microsoft/recommenders xDeepFM](../../recommenders/models/deeprec/models/xDeepFM.py)|Python|Support flexible interface with different configurations of FM and FM extensions, i.e., LR, FM, and/or CIN|[notebook](../00_quick_start/xdeepfm_criteo.ipynb) / [utilities](../../recommenders/models/deeprec/models/xDeepFM.py)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than `libfm` and `libffm`, all the other three can be used in a Python environment. \n",
    "\n",
    "* A deep dive of using Vowbal Wabbit for FM model can be found [here](../02_model_content_based_filtering/vowpal_wabbit_deep_dive.ipynb)\n",
    "* A quick start of Microsoft xDeepFM algorithm can be found [here](../00_quick_start/xdeepfm_criteo.ipynb). \n",
    "\n",
    "Therefore, in the example below, only code examples and best practices of using `xlearn` are presented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 xlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setups for using `xlearn`.\n",
    "\n",
    "1. `xlearn` is implemented in C++ and has Python bindings, so it can be directly installed as a Python package from PyPI. The installation of `xlearn` is enabled in the [Recommenders repo environment setup script](../../tools/generate_conda_file.py). One can follow the general setup steps to install the environment as required, in which `xlearn` is installed as well.\n",
    "2. NOTE `xlearn` may require some base libraries installed as prerequisites in the system, e.g., `cmake`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a succesful creation of the environment, one can load the packages to run `xlearn` in a Jupyter notebook or Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.13 |Anaconda, Inc.| (default, Feb 23 2021, 12:58:59) \n",
      "[GCC Clang 10.0.0 ]\n",
      "Xlearn version: 0.4.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "from tempfile import TemporaryDirectory\n",
    "import xlearn as xl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from recommenders.utils.constants import SEED\n",
    "from recommenders.utils.timer import Timer\n",
    "from recommenders.datasets.download_utils import maybe_download, unzip_file\n",
    "from recommenders.tuning.parameter_sweep import generate_param_grid\n",
    "from recommenders.datasets.pandas_df_utils import LibffmConverter\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Xlearn version: {}\".format(xl.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.doubanio.com/simple/\n",
      "Requirement already satisfied: recommenders in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (1.1.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.19 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.19.5)\n",
      "Requirement already satisfied: pandera[strategies]>=0.6.5 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (0.6.5)\n",
      "Requirement already satisfied: jinja2<3.1,>=2 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (3.0.3)\n",
      "Requirement already satisfied: lightfm<2,>=1.15 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.17)\n",
      "Requirement already satisfied: lightgbm>=2.2.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (3.3.5)\n",
      "Requirement already satisfied: matplotlib<4,>=2.2.2 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (3.3.4)\n",
      "Requirement already satisfied: bottleneck<2,>=1.2.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.3.7)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.3.4)\n",
      "Requirement already satisfied: pyyaml<6,>=5.4.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (5.4.1)\n",
      "Requirement already satisfied: cornac<2,>=1.1.2 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.15.1)\n",
      "Requirement already satisfied: pandas<2,>1.0.3 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.1.5)\n",
      "Requirement already satisfied: nltk<4,>=3.4 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (3.6.7)\n",
      "Requirement already satisfied: transformers<5,>=2.5.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (4.18.0)\n",
      "Requirement already satisfied: numba<1,>=0.38.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (0.53.1)\n",
      "Requirement already satisfied: scikit-surprise>=1.0.6 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.1.1)\n",
      "Requirement already satisfied: seaborn<1,>=0.8.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (0.11.2)\n",
      "Requirement already satisfied: memory-profiler<1,>=0.54.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (0.61.0)\n",
      "Requirement already satisfied: category-encoders<2,>=1.3.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.3.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.31.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (4.64.1)\n",
      "Requirement already satisfied: scipy<2,>=1.0.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (1.5.4)\n",
      "Requirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from recommenders) (0.24.2)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from category-encoders<2,>=1.3.0->recommenders) (0.12.2)\n",
      "Requirement already satisfied: patsy>=0.4.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.3)\n",
      "Requirement already satisfied: powerlaw in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from cornac<2,>=1.1.2->recommenders) (1.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
      "Requirement already satisfied: wheel in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from lightgbm>=2.2.1->recommenders) (0.37.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from matplotlib<4,>=2.2.2->recommenders) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from matplotlib<4,>=2.2.2->recommenders) (8.4.0)\n",
      "Requirement already satisfied: psutil in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.9.5)\n",
      "Requirement already satisfied: joblib in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages/click-8.1.3-py3.6.egg (from nltk<4,>=3.4->recommenders) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages/regex-2022.7.25-py3.6-macosx-10.9-x86_64.egg (from nltk<4,>=3.4->recommenders) (2022.7.25)\n",
      "Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from numba<1,>=0.38.1->recommenders) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from numba<1,>=0.38.1->recommenders) (58.0.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from pandas<2,>1.0.3->recommenders) (2022.2.1)\n",
      "Requirement already satisfied: wrapt in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from pandera[strategies]>=0.6.5->recommenders) (1.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.6.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from pandera[strategies]>=0.6.5->recommenders) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from pandera[strategies]>=0.6.5->recommenders) (4.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from pandera[strategies]>=0.6.5->recommenders) (21.3)\n",
      "Requirement already satisfied: hypothesis>=5.41.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from pandera[strategies]>=0.6.5->recommenders) (6.31.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from requests<3,>=2.0.0->recommenders) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from requests<3,>=2.0.0->recommenders) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
      "Requirement already satisfied: six>=1.7.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from retrying>=1.3.3->recommenders) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
      "Requirement already satisfied: importlib-resources in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from tqdm<5,>=4.31.1->recommenders) (5.2.3)\n",
      "Requirement already satisfied: filelock in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from transformers<5,>=2.5.0->recommenders) (3.4.1)\n",
      "Requirement already satisfied: dataclasses in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from transformers<5,>=2.5.0->recommenders) (0.8)\n",
      "Requirement already satisfied: sacremoses in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from transformers<5,>=2.5.0->recommenders) (0.0.53)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from transformers<5,>=2.5.0->recommenders) (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages/tokenizers-0.12.1-py3.6-macosx-10.9-x86_64.egg (from transformers<5,>=2.5.0->recommenders) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from transformers<5,>=2.5.0->recommenders) (4.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (21.4.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from typing-inspect>=0.6.0->pandera[strategies]>=0.6.5->recommenders) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.6.0)\n",
      "Requirement already satisfied: mpmath in /Users/conan/opt/anaconda3/envs/py36-tf1.13/lib/python3.6/site-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install recommenders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the FM model building, data is usually represented in the libsvm data format. That is, `label feat1:val1 feat2:val2 ...`, where `label` is the target to predict, and `val` is the value to each feature `feat`.\n",
    "\n",
    "FFM algorithm requires data to be represented in the libffm format, where each vector is split into several fields with categorical/numerical features inside. That is, `label field1:feat1:val1 field2:feat2:val2 ...`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Microsoft/Recommenders utility functions, [a libffm converter](../../recommenders/dataset/pandas_df_utils.py) is provided to achieve the transformation from a tabular feature vectors to the corresponding libffm representation. For example, the following shows how to transform the format of a synthesized data by using the module of `LibffmConverter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>field1</th>\n",
       "      <th>field2</th>\n",
       "      <th>field3</th>\n",
       "      <th>field4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1:1:1</td>\n",
       "      <td>2:4:3</td>\n",
       "      <td>3:5:1.0</td>\n",
       "      <td>4:6:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1:2:1</td>\n",
       "      <td>2:4:4</td>\n",
       "      <td>3:5:2.0</td>\n",
       "      <td>4:7:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1:3:1</td>\n",
       "      <td>2:4:5</td>\n",
       "      <td>3:5:3.0</td>\n",
       "      <td>4:8:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1:3:1</td>\n",
       "      <td>2:4:6</td>\n",
       "      <td>3:5:4.0</td>\n",
       "      <td>4:9:1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1:3:1</td>\n",
       "      <td>2:4:7</td>\n",
       "      <td>3:5:5.0</td>\n",
       "      <td>4:10:1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating field1 field2   field3  field4\n",
       "0       1  1:1:1  2:4:3  3:5:1.0   4:6:1\n",
       "1       0  1:2:1  2:4:4  3:5:2.0   4:7:1\n",
       "2       0  1:3:1  2:4:5  3:5:3.0   4:8:1\n",
       "3       1  1:3:1  2:4:6  3:5:4.0   4:9:1\n",
       "4       1  1:3:1  2:4:7  3:5:5.0  4:10:1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_feature_original = pd.DataFrame({\n",
    "    'rating': [1, 0, 0, 1, 1],\n",
    "    'field1': ['xxx1', 'xxx2', 'xxx4', 'xxx4', 'xxx4'],\n",
    "    'field2': [3, 4, 5, 6, 7],\n",
    "    'field3': [1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    'field4': ['1', '2', '3', '4', '5']\n",
    "})\n",
    "\n",
    "converter = LibffmConverter().fit(df_feature_original, col_rating='rating')\n",
    "df_out = converter.transform(df_feature_original)\n",
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 4 fields and 10 features.\n"
     ]
    }
   ],
   "source": [
    "print('There are in total {0} fields and {1} features.'.format(converter.field_count, converter.feature_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of `xlearn`, the following example uses the [Criteo data set](https://labs.criteo.com/category/dataset/), which has already been processed in the libffm format, for building and evaluating a FFM model built by using `xlearn`. Sometimes, it is important to know the total numbers of fields and features. When building a FFM model, `xlearn` can count these numbers automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "LEARNING_RATE = 0.2\n",
    "LAMBDA = 0.002\n",
    "EPOCH = 10\n",
    "OPT_METHOD = \"sgd\" # options are \"sgd\", \"adagrad\" and \"ftrl\"\n",
    "\n",
    "# The metrics for binary classification options are \"acc\", \"prec\", \"f1\" and \"auc\"\n",
    "# for regression, options are \"rmse\", \"mae\", \"mape\"\n",
    "METRIC = \"auc\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10.3k/10.3k [00:06<00:00, 1.51kKB/s]\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "YAML_FILE_NAME = \"xDeepFM.yaml\"\n",
    "TRAIN_FILE_NAME = \"cretio_tiny_train\"\n",
    "VALID_FILE_NAME = \"cretio_tiny_valid\"\n",
    "TEST_FILE_NAME = \"cretio_tiny_test\"\n",
    "MODEL_FILE_NAME = \"model.out\"\n",
    "OUTPUT_FILE_NAME = \"output.txt\"\n",
    "\n",
    "tmpdir = TemporaryDirectory()\n",
    "\n",
    "data_path = tmpdir.name\n",
    "yaml_file = os.path.join(data_path, YAML_FILE_NAME)\n",
    "train_file = os.path.join(data_path, TRAIN_FILE_NAME)\n",
    "valid_file = os.path.join(data_path, VALID_FILE_NAME)\n",
    "test_file = os.path.join(data_path, TEST_FILE_NAME)\n",
    "model_file = os.path.join(data_path, MODEL_FILE_NAME)\n",
    "output_file = os.path.join(data_path, OUTPUT_FILE_NAME)\n",
    "\n",
    "assets_url = \"https://recodatasets.z20.web.core.windows.net/deeprec/xdeepfmresources.zip\"\n",
    "assets_file = maybe_download(assets_url, work_directory=data_path)\n",
    "unzip_file(assets_file, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are from the [official documentation of `xlearn`](https://xlearn-doc.readthedocs.io/en/latest/index.html) for building a model. To begin with, we do not modify any training parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE, if `xlearn` is run through command line, the training process can be displayed in the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 22.3204\n"
     ]
    }
   ],
   "source": [
    "# Training task\n",
    "ffm_model = xl.create_ffm()        # Use field-aware factorization machine (ffm)\n",
    "ffm_model.setTrain(train_file)     # Set the path of training dataset\n",
    "ffm_model.setValidate(valid_file)  # Set the path of validation dataset\n",
    "\n",
    "# Parameters:\n",
    "#  0. task: binary classification\n",
    "#  1. learning rate: 0.2\n",
    "#  2. regular lambda: 0.002\n",
    "#  3. evaluation metric: auc\n",
    "#  4. number of epochs: 10\n",
    "#  5. optimization method: sgd\n",
    "param = {\"task\":\"binary\", \n",
    "         \"lr\": LEARNING_RATE, \n",
    "         \"lambda\": LAMBDA, \n",
    "         \"metric\": METRIC,\n",
    "         \"epoch\": EPOCH,\n",
    "         \"opt\": OPT_METHOD\n",
    "        }\n",
    "\n",
    "# Start to train\n",
    "# The trained model will be stored in model.out\n",
    "with Timer() as time_train:\n",
    "    ffm_model.fit(param, model_file)\n",
    "print(f\"Training time: {time_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction time: 1.5054\n"
     ]
    }
   ],
   "source": [
    "# Prediction task\n",
    "ffm_model.setTest(test_file)  # Set the path of test dataset\n",
    "ffm_model.setSigmoid()        # Convert output to 0-1\n",
    "\n",
    "# Start to predict\n",
    "# The output result will be stored in output.txt\n",
    "with Timer() as time_predict:\n",
    "    ffm_model.predict(model_file, output_file)\n",
    "print(f\"Prediction time: {time_predict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output are the predicted labels (i.e., 1 or 0) for the testing data set. AUC score is calculated to evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file) as f:\n",
    "    predictions = f.readlines()\n",
    "\n",
    "with open(test_file) as f:\n",
    "    truths = f.readlines()\n",
    "\n",
    "truths = np.array([float(truth.split(' ')[0]) for truth in truths])\n",
    "predictions = np.array([float(prediction.strip('')) for prediction in predictions])\n",
    "\n",
    "auc_score = roc_auc_score(truths, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7496594895319695"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/scrapbook.scrap.json+json": {
       "data": 0.7496594895319695,
       "encoder": "json",
       "name": "auc_score",
       "version": 1
      }
     },
     "metadata": {
      "scrapbook": {
       "data": true,
       "display": false,
       "name": "auc_score"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sb.glue('auc_score', auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the model building/scoring process is fast and the model performance is good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Hyperparameter tuning of `xlearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following presents a naive approach to tune the parameters of `xlearn`, which is using grid-search of parameter values to find the optimal combinations. It is worth noting that the original [FFM paper](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf) gave some hints in terms of the impact of parameters on the sampled Criteo dataset. \n",
    "\n",
    "The following are the parameters that can be tuned in the `xlearn` implementation of FM/FFM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter|Description|Default value|Notes|\n",
    "|-------------|-----------------|------------------|-----------------|\n",
    "|`lr`|Learning rate|0.2|Higher learning rate helps fit a model more efficiently but may also result in overfitting.|\n",
    "|`lambda`|Regularization parameter|0.00002|The value needs to be selected empirically to avoid overfitting.|\n",
    "|`k`|Dimensionality of the latent factors|4|In FFM the effect of k is not that significant as the algorithm itself considers field where `k` can be small to capture the effect of features within each of the fields.|\n",
    "|`init`|Model initialization|0.66|-|\n",
    "|`epoch`|Number of epochs|10|Using a larger epoch size will help converge the model to its optimal point|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {\n",
    "    \"lr\": [0.0001, 0.001, 0.01],\n",
    "    \"lambda\": [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "param_grid = generate_param_grid(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "\n",
    "with Timer() as time_tune:\n",
    "    for param in param_grid:\n",
    "        ffm_model = xl.create_ffm()       \n",
    "        ffm_model.setTrain(train_file)     \n",
    "        ffm_model.setValidate(valid_file)\n",
    "        ffm_model.fit(param, model_file)\n",
    "\n",
    "        ffm_model.setTest(test_file)  \n",
    "        ffm_model.setSigmoid()        \n",
    "        ffm_model.predict(model_file, output_file)\n",
    "\n",
    "        with open(output_file) as f:\n",
    "            predictions = f.readlines()\n",
    "\n",
    "        with open(test_file) as f:\n",
    "            truths = f.readlines()\n",
    "\n",
    "        truths = np.array([float(truth.split(' ')[0]) for truth in truths])\n",
    "        predictions = np.array([float(prediction.strip('')) for prediction in predictions])\n",
    "\n",
    "        auc_scores.append(roc_auc_score(truths, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tuning by grid search takes {0:.2} min'.format(time_tune.interval / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_scores = [float('%.4f' % x) for x in auc_scores]\n",
    "auc_scores_array = np.reshape(auc_scores, (len(param_dict[\"lr\"]), len(param_dict[\"lambda\"]))) \n",
    "\n",
    "auc_df = pd.DataFrame(\n",
    "    data=auc_scores_array, \n",
    "    index=pd.Index(param_dict[\"lr\"], name=\"LR\"), \n",
    "    columns=pd.Index(param_dict[\"lambda\"], name=\"Lambda\")\n",
    ")\n",
    "auc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.heatmap(auc_df, cbar=False, annot=True, fmt=\".4g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced tuning methods like Bayesian Optimization can be used for searching for the optimal model efficiently. The benefit of using, for example, `HyperDrive` from Azure Machine Learning Services, for tuning the parameters, is that, the tuning tasks can be distributed across nodes of a cluster and the optimization can be run concurrently to save the total cost.\n",
    "\n",
    "* Details about how to tune hyper parameters by using Azure Machine Learning Services can be found [here](https://github.com/microsoft/recommenders/tree/master/notebooks/04_model_select_and_optimize).\n",
    "* Note, to enable the tuning task on Azure Machine Learning Services by using HyperDrive, one needs a Docker image to containerize the environment where `xlearn` can be run. The Docker file provided [here](https://github.com/microsoft/recommenders/tree/master/docker) can be used for such purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpdir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "1. Rendle, Steffen. \"Factorization machines.\" 2010 IEEE International Conference on Data Mining. IEEE, 2010.\n",
    "2. Juan, Yuchin, et al. \"Field-aware factorization machines for CTR prediction.\" Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 2016.\n",
    "3. Guo, Huifeng, et al. \"DeepFM: a factorization-machine based neural network for CTR prediction.\" arXiv preprint arXiv:1703.04247, 2017.\n",
    "4. Lian, Jianxun, et al. \"xdeepfm: Combining explicit and implicit feature interactions for recommender systems.\" Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2018.\n",
    "5. Qu, Yanru, et al. \"Product-based neural networks for user response prediction.\" 2016 IEEE 16th International Conference on Data Mining (ICDM). IEEE, 2016.\n",
    "6. Zhang, Weinan, Tianming Du, and Jun Wang. \"Deep learning over multi-field categorical data.\" European conference on information retrieval. Springer, Cham, 2016.\n",
    "7. He, Xiangnan, and Tat-Seng Chua. \"Neural factorization machines for sparse predictive analytics.\" Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 2017.\n",
    "8. Cheng, Heng-Tze, et al. \"Wide & deep learning for recommender systems.\" Proceedings of the 1st workshop on deep learning for recommender systems. ACM, 2016.\n",
    "9. Langford, John, Lihong Li, and Alex Strehl. \"Vowpal wabbit online learning project.\", 2007."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
